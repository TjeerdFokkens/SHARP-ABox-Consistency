import pyactr as actr
import pandas as pd
import sys
import os
import matplotlib.pyplot as plt
import numpy as np
import Module1 as md1
import Module2 as md2
import Module3 as md3
import Module4 as md4
import Module5 as md5
#import parser as par
from matplotlib import colors
from matplotlib.ticker import PercentFormatter
import re
from matplotlib.ticker import MaxNLocator
from collections import Counter
from scipy import stats
import matplotlib as mpl
import matplotlib.gridspec as grid_spec
from sklearn.neighbors import KernelDensity
from sklearn import linear_model
import matplotlib.patches as patches
from scipy.optimize import curve_fit
import statsmodels.api as sm
from lark import Lark, Transformer, Visitor, v_args
from scipy import stats


dat = pd.read_csv('/Users/xfoktj/Documents/GitHub/ABox-Consistency/data/data18.csv',index_col=False, usecols = ['ABox','Run','Time','Judgement'])
sam_num = dat['ABox'].nunique()
print(dat['ABox'].unique())

#For every run of every ABox, remove the outliers more than d standard deviations away.
d = 3
df = pd.DataFrame()
for abox in dat['ABox'].unique():
    for run in dat['Run'].unique():
        holder1 = dat[dat['ABox']==abox]
        holder2 = holder1[holder1['Run']==run]
        if holder2.empty == False:
            holder3 = holder2[(np.abs(stats.zscore(holder2['Time'])) < d)]
            df = df.append(holder3, ignore_index=True)
            dat[((dat['ABox']==abox) & (dat['Run']==run))] = holder3
        else:
            continue
dat = df

dat = dat[dat['ABox']=='a:(A&(B&C)),a:(D&(E&-C))']



def lowerr(sam1, sam2):
    #For every value in sam1, count how many values in sam2 are strictly smaller.
    li1=[]
    for i in sam1:
        v = len(list(x for x in sam2 if x < i))
        li1.append(v)
    return li1

def samee(sam1, sam2):
    #For every value in sam1, count how many values are equally large.
    li2=[]
    for i in sam1:
        v = len(list(x for x in sam2 if x == i))
        li2.append(v)
    return li2

def A_w(sam1, sam2):
    #Calculate the probability of superiority.
    l = lowerr(sam1,sam2)
    s = samee(sam1,sam2)
    numerator = sum(l)+0.5*sum(s)
    denominator = sam1.size * sam2.size
    fraction = numerator/denominator
    return round(fraction,3)

def cohens_d(sample1,sample2):
    s1 = sample1.std()
    s2 = sample2.std()
    s = np.sqrt((s1**2 + s2**2)/2)
    m1 = sample1.mean()
    m2 = sample2.mean()
    d = (m2-m1)/s
    return round(d,3)

#The primary statistics:
def stat_primary(df,abox,d):
    print('For the ABox ', abox, ' we get the following.')
    dat = df[df['ABox']==abox]
    dat = dat[(np.abs(stats.zscore(dat['Time'])) < d)] #Removing outliers
    dat2 = dat.groupby(['Run']).mean()
    time_min = dat2['Time'].min()
    print('The minimum time:',round(time_min,3))
    time_avg = dat['Time'].mean()
    print('The average time:',round(time_avg,3))
    time_max = dat2['Time'].max()
    print('The maximum time:',round(time_max,3))
    run_efficient = dat2[dat2['Time']==time_min].index[0]
    print('The most efficient run:',run_efficient)
    time_average_efficient = dat[dat['Run']==run_efficient]['Time'].mean()
    print('Average time of the most efficient run:',round(time_average_efficient,3))
    run_leastefficient = dat2[dat2['Time']==time_max].index[0]
    print('The least efficient run:',run_leastefficient)
    print('The probability of superiority between most and least efficient:',A_w(dat[dat['Run']==run_efficient]['Time'], dat[dat['Run']==run_leastefficient]['Time']))
    print('')
    return time_avg
'''
data = []
for abox in dat['ABox'].unique():
    t = stat_primary(dat,abox,3.0)
    data.append(t)
print(data)
'''
def compare(abox1,abox2,dat):
    sam1 = dat[dat['ABox']==abox1]['Time']
    sam2 = dat[dat['ABox']==abox2]['Time']
    Aw = A_w(sam1, sam2)
    print('The probability of superiority of ', abox1, ' over ', abox2, ' is: ',Aw)
    d = cohens_d(sam1,sam2)
    print('The effect size in terms of Cohens d: ', d)
    diff = sam2.mean()-sam1.mean()
    print('The absolute difference of the means: ', round(diff,3))
    print('The relative difference of the means (compared to first sample): ', round((diff/sam1.mean())*100,1),'%')
'''
abox = 'a:/Er.(A&B),a:(C&/Ar.-A)'
sam1 = dat[(dat['Run']=='| -> a:(C&/Ar.-A) -> a:/Ar.-A -> a:/Er.(A&B) -> x1:(A&B) -> a:/Ar.-A') | (dat['Run']=='| -> a:(C&/Ar.-A) -> a:/Ar.-A -> a:/Er.(A&B) -> a:/Ar.-A -> x1:(A&B)')]['Time']
sam2 = dat[dat['Run']=='| -> a:/Er.(A&B) -> x1:(A&B) -> a:(C&/Ar.-A) -> a:/Ar.-A']['Time']
sam3 = dat[dat['Run']=='| -> a:(C&/Ar.-A) -> a:/Er.(A&B) -> x1:(A&B) -> a:/Ar.-A']['Time']
sam4 = dat[dat['Run']=='| -> a:/Er.(A&B) -> a:(C&/Ar.-A) -> a:/Ar.-A -> x1:(A&B)']['Time']
sam5 = dat[dat['Run']=='| -> a:(C&/Ar.-A) -> a:/Er.(A&B) -> a:/Ar.-A -> x1:(A&B)']['Time']
sam6 = dat[dat['Run']=='| -> a:/Er.(A&B) -> a:(C&/Ar.-A) -> x1:(A&B) -> a:/Ar.-A']['Time']
print(len(sam1))
print(len(sam2))
print(len(sam3))
print(len(sam4))
print(len(sam5))
print(len(sam6))
'''
def compareruns(sam1, name1, sam2, name2):
    Aw = A_w(sam1, sam2)
    print('The probability of superiority of ', name1, ' over ', name2, ' is: ',Aw)
    d = cohens_d(sam1,sam2)
    print('The effect size in terms of Cohens d: ', d)
    diff = sam2.mean()-sam1.mean()
    print('The absolute difference of the means: ', round(diff,3))
    print('The relative difference of the means (compared to first sample): ', round((diff/sam1.mean())*100,1),'%')
'''
[name1,name2,name3,name4,name5,name6] = ['noSwitcha','noSwitchb','1Switcha','1Switchb','2Switcha','2Switchb']
compareruns(sam1, name1, sam2, name2)
compareruns(sam1, name1, sam3, name3)
compareruns(sam2, name2, sam3, name3)
compareruns(sam4, name4, sam5, name5)
compareruns(sam4, name4, sam6, name6)
compareruns(sam5, name5, sam6, name6)
'''

#compare(dat['ABox'].unique()[2],dat['ABox'].unique()[3], dat)
#compare(dat['ABox'].unique()[4],dat['ABox'].unique()[5], dat)
#compare(dat['ABox'].unique()[0],dat['ABox'].unique()[1], dat)

'''
for sam in dat['ABox'].unique():
    sample = dat[dat['ABox']==sam]['Time']
    dev = sample.std()
    print('The standard deviation is:', dev)
    n = len(sample.index)
    b = 1.06 * dev * n**(-1/5)
    print('The optimal bandwidth: ',b)
'''

def graph(dat,bandwidth,title,save,R):
    #Determining the minimum and maximum x values of the graph.
    lx=dat['Time'].min() * 0.8 #the left x limit
    rx=dat['Time'].max() * 1.1 #the right x limit
    if R == False:
        samples = dat['ABox'].unique()
        sam_num = len(samples)
        #samples[2],samples[3],samples[4],samples[5] = samples[4],samples[5],samples[2],samples[3]
        print(samples)
        #Defining the colours
        colors = [mpl.cm.viridis(i) for i in np.linspace(0,1,10)]

        gs = grid_spec.GridSpec(dat['ABox'].nunique(),1)
        fig = plt.figure(figsize=(10,6))

        y_max=0
        x_d = np.linspace(lx,rx, 1000)
        logprob = []

        for sam in samples:
            kde = KernelDensity(bandwidth=bandwidth, kernel='gaussian')
            kde.fit(np.array(dat[dat['ABox']==sam]['Time'])[:,None])
            plot_log_data = kde.score_samples(x_d[:, None])
            plot_data = np.exp(plot_log_data)
            logprob.append(plot_data)
            y_max = max(y_max,max(plot_data))

        y_max = y_max * 1.10
        ax_objs = []
        i = 0
        dict = {}

        for sam in samples:

            ax_objs.append(fig.add_subplot(gs[i:i+1, 0:]))
            # plotting the distribution
            ax_objs[-1].plot(x_d, np.array(logprob[i]),color="#f0f0f0",lw=0.5)
            ax_objs[-1].fill_between(x_d, logprob[i], alpha=1,color=colors[i+2])

            # setting uniform x and y lims
            ax_objs[-1].set_xlim(lx,rx)
            ax_objs[-1].set_ylim(0,y_max)

            # make background transparent
            rect = ax_objs[-1].patch
            rect.set_alpha(0)

            # remove borders, axis ticks, and labels
            ax_objs[-1].set_yticklabels([])
            ax_objs[-1].yaxis.set_visible(False)

            if i == sam_num-1:
                ax_objs[-1].set_xlabel("Inference time (s)", fontsize=14,fontweight="bold")
            else:
                ax_objs[-1].set_xticklabels([])

            spines = ["top","right","left","bottom"]
            for s in spines:
                ax_objs[-1].spines[s].set_visible(False)

            st = '$\mathcal{A}' + '_' + '{}'.format(i) + '$'
            ax_objs[-1].text(lx-0.02,0,st,fontweight="bold",fontsize=15,ha="right")
            dict[st] = sam

            i += 1

        gs.update(hspace=-0.6)
        fig.text(0.07,0.85,"Distribution of inference times",fontsize=18)
        j = 0
        #for i in dict.keys():
            #st = i + ': ' + dict[i]
            #fig.text(0.7,0.8-0.03*j, st,fontsize=9, horizontalalignment='left', verticalalignment='top')
            #j += 1

        #fig.patches.extend([patches.FancyBboxPatch((0.69,0.79-j*0.03),0.4,0.4, fill=True, color='grey', alpha=0.25, zorder=0, transform=fig.transFigure, figure=fig, boxstyle = 'round,pad=0.01')])

        if save == True:
            plt.savefig(title, transparent=True, dpi=200)
        else:
            plt.show()
    else:
        samples = dat['Run'].unique()
        sam_num = len(samples)
        #samples[2],samples[3],samples[4],samples[5] = samples[4],samples[5],samples[2],samples[3]
        print(samples)
        #Defining the colours
        colors = [mpl.cm.viridis(i) for i in np.linspace(0,1,10)]

        gs = grid_spec.GridSpec(dat['Run'].nunique(),1)
        fig = plt.figure(figsize=(10,6))

        y_max=0
        x_d = np.linspace(lx,rx, 1000)
        logprob = []

        for sam in samples:
            kde = KernelDensity(bandwidth=bandwidth, kernel='gaussian')
            kde.fit(np.array(dat[dat['Run']==sam]['Time'])[:,None])
            plot_log_data = kde.score_samples(x_d[:, None])
            plot_data = np.exp(plot_log_data)
            logprob.append(plot_data)
            y_max = max(y_max,max(plot_data))

        y_max = y_max * 1.10
        ax_objs = []
        i = 0
        dict = {}

        for sam in samples:

            ax_objs.append(fig.add_subplot(gs[i:i+1, 0:]))
            # plotting the distribution
            ax_objs[-1].plot(x_d, np.array(logprob[i]),color="#f0f0f0",lw=0.5)
            ax_objs[-1].fill_between(x_d, logprob[i], alpha=1,color=colors[i+2])

            # setting uniform x and y lims
            ax_objs[-1].set_xlim(lx,rx)
            ax_objs[-1].set_ylim(0,y_max)

            # make background transparent
            rect = ax_objs[-1].patch
            rect.set_alpha(0)

            # remove borders, axis ticks, and labels
            ax_objs[-1].set_yticklabels([])
            ax_objs[-1].yaxis.set_visible(False)

            if i == sam_num-1:
                ax_objs[-1].set_xlabel("Inference time (s)", fontsize=14,fontweight="bold")
            else:
                ax_objs[-1].set_xticklabels([])

            spines = ["top","right","left","bottom"]
            for s in spines:
                ax_objs[-1].spines[s].set_visible(False)

            st = '$\mathbf{Run}' + '_' + '{}'.format(i) + '$'
            ax_objs[-1].text(lx-0.02,0,st,fontweight="bold",fontsize=12,ha="right")
            dict[st] = sam

            i += 1

        gs.update(hspace=-0.6)
        fig.text(0.07,0.85,"Distribution of inference times",fontsize=18)
        j = 0
        #for i in dict.keys():
            #st = i + ': ' + dict[i]
            #fig.text(0.7,0.8-0.03*j, st,fontsize=9, horizontalalignment='left', verticalalignment='top')
            #j += 1

        #fig.patches.extend([patches.FancyBboxPatch((0.69,0.79-j*0.03),0.4,0.4, fill=True, color='grey', alpha=0.25, zorder=0, transform=fig.transFigure, figure=fig, boxstyle = 'round,pad=0.01')])

        if save == True:
            plt.savefig(title, transparent=True, dpi=200)
        else:
            plt.show()

#graph(dat,0.05,'DifferentRun1.png',False,True)

def graphruns(dat,bandwidth,title,save):
    d = 3
    sam0 = dat[(dat['Run']=='| -> a:(D&(E&-C)) -> a:(E&-C) -> a:(A&(B&C)) -> a:(B&C)') | (dat['Run']=='| -> a:(A&(B&C)) -> a:(B&C) -> a:(D&(E&-C)) -> a:(E&-C)')]['Time']
    sam1 = dat[(dat['Run']=='| -> a:(D&(E&-C)) -> a:(A&(B&C)) -> a:(B&C) -> a:(E&-C)') | (dat['Run']=='| -> a:(A&(B&C)) -> a:(D&(E&-C)) -> a:(E&-C) -> a:(B&C)')]['Time']
    sam2 = dat[(dat['Run']=='| -> a:(A&(B&C)) -> a:(D&(E&-C)) -> a:(B&C) -> a:(E&-C)') | (dat['Run']=='| -> a:(D&(E&-C)) -> a:(A&(B&C)) -> a:(E&-C) -> a:(B&C)')]['Time']
    print('The sample of zero switches has size: ', len(sam0))
    print('The sample of one switch has size: ', len(sam1))
    print('The sample of two switches has size: ', len(sam2))
    samples = [sam0,sam1,sam2] # The runs
    names = ['0 Switches','1 Switch','2 Switches']
    #Determining the minimum and maximum x values of the graph.
    lx=dat['Time'].min() * 1.1 #the left x limit
    rx=dat['Time'].max() * 1.1 #the right x limit

    #Defining the colours
    colors = [mpl.cm.viridis(i) for i in np.linspace(0,1,10)]

    gs = grid_spec.GridSpec(len(samples),1)
    fig = plt.figure(figsize=(10,6))

    y_max=0
    x_d = np.linspace(lx,rx, 1000)
    logprob = []


    for sam in samples:
        kde = KernelDensity(bandwidth=bandwidth, kernel='gaussian')
        kde.fit(np.array(sam)[:,None])
        plot_log_data = kde.score_samples(x_d[:, None])
        plot_data = np.exp(plot_log_data)
        logprob.append(plot_data)
        y_max = max(y_max,max(plot_data))

    y_max = y_max * 1.10
    ax_objs = []
    i = 0
    dict = {}
    st = ['0 Switches', '1 Switch', '2 Switches']
    for sam in samples:

        ax_objs.append(fig.add_subplot(gs[i:i+1, 0:]))
        # plotting the distribution
        ax_objs[-1].plot(x_d, np.array(logprob[i]),color="#f0f0f0",lw=0.5)
        ax_objs[-1].fill_between(x_d, logprob[i], alpha=1,color=colors[i+2])

        # setting uniform x and y lims
        ax_objs[-1].set_xlim(lx,rx)
        ax_objs[-1].set_ylim(0,y_max)

        # make background transparent
        rect = ax_objs[-1].patch
        rect.set_alpha(0)

        # remove borders, axis ticks, and labels
        ax_objs[-1].set_yticklabels([])
        ax_objs[-1].yaxis.set_visible(False)

        if i == len(samples)-1:
            ax_objs[-1].set_xlabel("Inference time (s)", fontsize=14,fontweight="bold")
        else:
            ax_objs[-1].set_xticklabels([])

        spines = ["top","right","left","bottom"]
        for s in spines:
            ax_objs[-1].spines[s].set_visible(False)

        ax_objs[-1].text(lx-0.02,0,st[i],fontweight="bold",fontsize=10,ha="right")
        dict[st[i]] = names[i]

        i += 1

    gs.update(hspace=-0.6)
    fig.text(0.07,0.85,"Distribution of inference times",fontsize=18)
    j = 0
    if save == True:
        plt.savefig(title, transparent=True, dpi=200)
    else:
        plt.show()
    return

#graphruns(dat,0.05,'DifferentRun1.png',True)

def graphrunsb(dat,bandwidth,title,save):
    d = 2.5
    sam1 = dat[dat['Run']=='| -> a:/Er.(A&B) -> x1:(A&B) -> a:(C&/Ar.-A) -> a:/Ar.-A']['Time']
    sam1 = sam1[(np.abs(stats.zscore(sam1)) < d)] #Removing the outliers
    sam2 = dat[dat['Run']=='| -> a:/Er.(A&B) -> a:(C&/Ar.-A) -> a:/Ar.-A -> x1:(A&B)']['Time']
    sam2 = sam2[(np.abs(stats.zscore(sam2)) < d)]
    sam3 = dat[dat['Run']=='| -> a:/Er.(A&B) -> a:(C&/Ar.-A) -> x1:(A&B) -> a:/Ar.-A']['Time']
    sam3 = sam3[(np.abs(stats.zscore(sam3)) < d)]
    sam4 = dat[dat['Run']=='| -> a:(C&/Ar.-A) -> a:/Ar.-A -> a:/Er.(A&B) -> x1:(A&B) -> a:/Ar.-A']['Time']
    sam4 = sam4[(np.abs(stats.zscore(sam4)) < d)]
    sam5 = dat[dat['Run']=='| -> a:(C&/Ar.-A) -> a:/Ar.-A -> a:/Er.(A&B) -> a:/Ar.-A -> x1:(A&B)']['Time']
    sam5 = sam5[(np.abs(stats.zscore(sam5)) < d)]
    sam6 = dat[dat['Run']=='| -> a:(C&/Ar.-A) -> a:/Er.(A&B) -> x1:(A&B) -> a:/Ar.-A']['Time']
    sam6 = sam6[(np.abs(stats.zscore(sam6)) < d)]
    sam7 = dat[dat['Run']=='| -> a:(C&/Ar.-A) -> a:/Er.(A&B) -> a:/Ar.-A -> x1:(A&B)']['Time']
    sam7 = sam7[(np.abs(stats.zscore(sam7)) < d)]
    samples = [sam1,sam2,sam3,sam4,sam5,sam6,sam7] # The runs
    for sam in samples:
        print(len(sam))
    names = ['noSwitch-a','1Switch-a','2Switch-a','noSwitch-c','1Switch-c','1Switch-b','2Switch-b']
    #Determining the minimum and maximum x values of the graph.
    lx=dat['Time'].min() * 2.5 #the left x limit
    rx=dat['Time'].max() * 1.1 #the right x limit

    #Defining the colours
    colors = [mpl.cm.viridis(i) for i in np.linspace(0,1,10)]

    gs = grid_spec.GridSpec(len(samples),1)
    fig = plt.figure(figsize=(10,6))

    y_max=0
    x_d = np.linspace(lx,rx, 1000)
    logprob = []


    for sam in samples:
        kde = KernelDensity(bandwidth=bandwidth, kernel='gaussian')
        kde.fit(np.array(sam)[:,None])
        plot_log_data = kde.score_samples(x_d[:, None])
        plot_data = np.exp(plot_log_data)
        logprob.append(plot_data)
        y_max = max(y_max,max(plot_data))

    y_max = y_max * 1.10
    ax_objs = []
    i = 0
    dict = {}

    for sam in samples:

        ax_objs.append(fig.add_subplot(gs[i:i+1, 0:]))
        # plotting the distribution
        ax_objs[-1].plot(x_d, np.array(logprob[i]),color="#f0f0f0",lw=0.5)
        ax_objs[-1].fill_between(x_d, logprob[i], alpha=1,color=colors[i+2])

        # setting uniform x and y lims
        ax_objs[-1].set_xlim(lx,rx)
        ax_objs[-1].set_ylim(0,y_max)

        # make background transparent
        rect = ax_objs[-1].patch
        rect.set_alpha(0)

        # remove borders, axis ticks, and labels
        ax_objs[-1].set_yticklabels([])
        ax_objs[-1].yaxis.set_visible(False)

        if i == len(samples)-1:
            ax_objs[-1].set_xlabel("Inference time (s)", fontsize=14,fontweight="bold")
        else:
            ax_objs[-1].set_xticklabels([])

        spines = ["top","right","left","bottom"]
        for s in spines:
            ax_objs[-1].spines[s].set_visible(False)

        st = 'Run {}'.format(i)
        ax_objs[-1].text(lx-0.02,0,st,fontweight="bold",fontsize=10,ha="right")
        dict[st] = names[i]

        i += 1

    gs.update(hspace=-0.6)
    fig.text(0.07,0.85,"Distribution of inference times",fontsize=18)
    j = 0
    for i in dict.keys():
        st = i + ': ' + dict[i]
        fig.text(0.8,0.8-0.03*j, st,fontsize=9, horizontalalignment='left', verticalalignment='top')
        j += 1

    fig.patches.extend([plt.Rectangle((0.78,0.81-(len(names) + 1)*0.03),0.4,0.5,fill=True, color='grey', alpha=0.3, zorder=1000,transform=fig.transFigure, figure=fig)])

    if save == True:
        plt.savefig(title, transparent=True, dpi=200)
    else:
        plt.show()
    return



'''
#Multiple linear regression
x = np.array([[0,2,2,1,1,1,1,1,2,1,0,0,2,1,1,1],[0,0,2,0,0,1,0,0,0,2,1,0,0,1,0,0],[0,0,0,2,0,1,1,1,0,0,1,2,1,1,1,3],[1,1,0,1,1,1,1,1,0,0,0,0,0,0,0,0]]).transpose()
y = dat.groupby(['ABox']).min()['Time']
n = len(y)
p = 4
print(y)

regr = linear_model.LinearRegression()
regr.fit(x, y)

print('Intercept: \n', regr.intercept_)
print('Coefficients: \n', regr.coef_)
print(regr.score(x, y))
print('The adjusted R2: ', 1-(1-regr.score(x, y))*(n-1)/(n-p-1))
print('predictions: ', regr.predict(x))
'''
[1,1,0,1,1,1,1,1,0,0,0,0,0,0,0,0]

# the presence of clashes: [1,0,1,0,1,0,1,0,1,0,1,0,1,0,1,0], [[1,1,2,2,1,1,2,2,0,0,0,0,0,0,1,1],[0,0,0,0,0,0,0,0,0,0,0,0,0,0,1,1],[0,0,0,0,0,0,1,1,1,1,2,2,0,0,1,1]]
#--------------------------------------------------------------

'''
dat = pd.read_csv('/Users/xfoktj/Documents/GitHub/ABox-Consistency/data/data11.csv')
dat2 = pd.read_csv('/Users/xfoktj/Documents/GitHub/ABox-Consistency/data/data13.csv')
com_list = [5,5,5,5,5,5,5,5,5,5,5,5,5,5,5,5,5,5,5,5,12,12,12,12,12,12,12,12,12,12,12,12,12,12,12,12,12,12,12,12,19,19,19,19,19,19,19,19,19,19,19,19,19,19,19,19,19,19,19,19]
com_list2 = [9,9,9,9,9,9,9,9,9,9,9,9,9,9,9,9,9,9,9,9,16,16,16,16,16,16,16,16,16,16,16,16,16,16,16,16,16,16,16,16]
dat['Complexity'] = com_list
dat2['Complexity'] = com_list2

dat = dat.append(dat2)

#Make the graph showing the exponential increase in run time with the input size.
x = list(dat['Complexity'])
y = list(dat['Time'])
xdata = np.linspace(2,20,50)
def func(x,a,b):
    return a * np.exp(b * x)
popt,pcov = curve_fit(func, x, y)
plt.plot(xdata, func(xdata, *popt), 'r-', label='fit')
print(popt)
li = [func(i, *popt) for i in x]
a = li
for i in range(len(li)):
    a[i] = (y[i]-li[i])**2
y_mean = sum(y)/len(y)
y_dif = [(i-y_mean)**2 for i in y]
R2=1-sum(a)/sum(y_dif)
print(R2)
plt.scatter(x, y, s=15)
plt.xlabel('Size of ABox')
plt.ylabel('Inference time (s)')
plt.title('Inference time scales exponentially with ABox size')
#plt.savefig('ExponentialIncreaseInference.png', transparent=True, dpi=200)
plt.show()
'''


#x = [5,9,12,16,19] #The complexities of the ABoxes for exponential scaling.
#yy = [41.987,99.997,247.63,593.989,1304.344] #The simulation times for exponential scaling

#x = [1,2,3,4,5] #The complexities of the ABoxes for linear scaling.
#yy = [16.7,18.4,21.5,25.4,29.0] #The simulation times for linear scaling

x = [1,2,7,10,13] #The complexities of the ABoxes for polynomial scaling.
yy = [16.7,22.0,33.9,54.1,80.7] #The simulation times for polynomial scaling
y = [i/10 for i in yy]
print(y)


#Make the graph showing the exponential increase in run time with the input size.
xdata = np.linspace(0,14,50)
def func(x,a,b,c):
    return a + b * x + c * x**2
popt,pcov = curve_fit(func, x, y)
plt.plot(xdata, func(xdata, *popt), 'r-', label='fit')
print(popt)
li = [func(i, *popt) for i in x]
a = li
for i in range(len(li)):
    a[i] = (y[i]-li[i])**2
y_mean = sum(y)/len(y)
y_dif = [(i-y_mean)**2 for i in y]
R2=1-sum(a)/sum(y_dif)
print(R2)
plt.scatter(x, y, s=15)
plt.xlabel('Size of ABox')
plt.ylabel('Simulation time (s)')
plt.title('Simulation time scales polynomially with ABox size')
plt.savefig('PolynomialIncrease.png', transparent=True, dpi=200)
#plt.show()
